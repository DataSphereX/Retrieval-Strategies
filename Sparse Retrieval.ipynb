{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Tokenization <Br>\n",
    "First, we need to tokenize the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'cat', 'sat', 'on', 'the', 'mat'], ['the', 'dog', 'sat', 'on', 'the', 'log'], ['the', 'cat', 'chased', 'the', 'dog']]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"The cat chased the dog\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_documents = [doc.lower().split() for doc in documents]\n",
    "print(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate Term Frequency (TF)\n",
    "Next, we calculate the term frequency for each term in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}), Counter({'the': 2, 'dog': 1, 'sat': 1, 'on': 1, 'log': 1}), Counter({'the': 2, 'cat': 1, 'chased': 1, 'dog': 1})]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculate term frequency for each document\n",
    "tf = [Counter(doc) for doc in tokenized_documents]\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "Now, we calculate the inverse document frequency for each term across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on': 0.4054651081081644, 'cat': 0.4054651081081644, 'the': 0.0, 'sat': 0.4054651081081644, 'mat': 1.0986122886681098, 'dog': 0.4054651081081644, 'log': 1.0986122886681098, 'chased': 1.0986122886681098}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Calculate document frequency for each term\n",
    "df = Counter()\n",
    "for doc in tokenized_documents:\n",
    "    df.update(set(doc))\n",
    "\n",
    "# Calculate IDF for each term\n",
    "idf = {term: math.log(len(documents) / df[term]) for term in df}\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate TF-IDF\n",
    "Multiply TF by IDF for each term in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'the': 0.0, 'cat': 0.4054651081081644, 'sat': 0.4054651081081644, 'on': 0.4054651081081644, 'mat': 1.0986122886681098}, {'the': 0.0, 'dog': 0.4054651081081644, 'sat': 0.4054651081081644, 'on': 0.4054651081081644, 'log': 1.0986122886681098}, {'the': 0.0, 'cat': 0.4054651081081644, 'chased': 1.0986122886681098, 'dog': 0.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF for each document\n",
    "tf_idf = []\n",
    "for doc_tf in tf:\n",
    "    doc_tf_idf = {term: freq * idf[term] for term, freq in doc_tf.items()}\n",
    "    tf_idf.append(doc_tf_idf)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Query Matching\n",
    "Given a query, calculate its TF-IDF and match it against the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0.4054651081081644, 'sat': 0.4054651081081644}\n",
      "[0.43976863279651823, 0.21988431639825912, 0.23135443112611218]\n",
      "The most relevant document is: The cat sat on the mat\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the query\n",
    "query = \"cat sat\"\n",
    "tokenized_query = query.lower().split()\n",
    "\n",
    "# Calculate term frequency for the query\n",
    "query_tf = Counter(tokenized_query)\n",
    "\n",
    "# Calculate TF-IDF for the query\n",
    "query_tf_idf = {term: freq * idf.get(term, 0) for term, freq in query_tf.items()}\n",
    "print(query_tf_idf)\n",
    "\n",
    "# Calculate cosine similarity between query and each document\n",
    "def cosine_similarity(doc_tf_idf, query_tf_idf):\n",
    "    dot_product = sum(doc_tf_idf.get(term, 0) * query_tf_idf.get(term, 0) for term in query_tf_idf)\n",
    "    doc_magnitude = math.sqrt(sum(value ** 2 for value in doc_tf_idf.values()))\n",
    "    query_magnitude = math.sqrt(sum(value ** 2 for value in query_tf_idf.values()))\n",
    "    if doc_magnitude == 0 or query_magnitude == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (doc_magnitude * query_magnitude)\n",
    "\n",
    "# Calculate similarity for each document\n",
    "similarities = [cosine_similarity(doc_tf_idf, query_tf_idf) for doc_tf_idf in tf_idf]\n",
    "print(similarities)\n",
    "\n",
    "# Find the most relevant document\n",
    "most_relevant_doc_index = similarities.index(max(similarities))\n",
    "print(f\"The most relevant document is: {documents[most_relevant_doc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG retrieval using Python with the transformers library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install Required Libraries\n",
    "First, install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Import Libraries and Load Models\n",
    "Import the required libraries and load the pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarve\\OneDrive\\Desktop\\LI\\Carosals\\Retrieval\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\sarve\\OneDrive\\Desktop\\LI\\Carosals\\Retrieval\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sarve\\.cache\\huggingface\\hub\\models--facebook--rag-token-nq. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\sarve\\OneDrive\\Desktop\\LI\\Carosals\\Retrieval\\venv\\Lib\\site-packages\\transformers\\models\\bart\\configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nRagRetriever requires the ðŸ¤— Datasets library but it was not found in your environment. You can install it with:\n```\npip install datasets\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install datasets\n```\nthen restarting your kernel.\n\nNote that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\nworking directory, python may try to import this instead of the ðŸ¤— Datasets library. You should rename this folder or\nthat python file if that's the case. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the tokenizer, retriever, and model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mRagRetriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/rag-token-nq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexact\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_dummy_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m RagTokenForGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/rag-token-nq\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39mretriever)\n",
      "File \u001b[1;32mc:\\Users\\sarve\\OneDrive\\Desktop\\LI\\Carosals\\Retrieval\\venv\\Lib\\site-packages\\transformers\\models\\rag\\retrieval_rag.py:442\u001b[0m, in \u001b[0;36mRagRetriever.from_pretrained\u001b[1;34m(cls, retriever_name_or_path, indexed_dataset, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, retriever_name_or_path, indexed_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 442\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfaiss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     config \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m RagConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    444\u001b[0m     rag_tokenizer \u001b[38;5;241m=\u001b[39m RagTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(retriever_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\sarve\\OneDrive\\Desktop\\LI\\Carosals\\Retrieval\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1678\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1676\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nRagRetriever requires the ðŸ¤— Datasets library but it was not found in your environment. You can install it with:\n```\npip install datasets\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install datasets\n```\nthen restarting your kernel.\n\nNote that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\nworking directory, python may try to import this instead of the ðŸ¤— Datasets library. You should rename this folder or\nthat python file if that's the case. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\n",
    "\n",
    "# Load the tokenizer, retriever, and model\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the Knowledge Base\n",
    "Define a small knowledge base for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"The cat chased the dog.\",\n",
    "    \"Cats are small, carnivorous mammals.\",\n",
    "    \"Dogs are domesticated mammals, not natural wild animals.\"\n",
    "]\n",
    "\n",
    "# Add the knowledge base to the retriever\n",
    "retriever.index.index_data(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Generate Responses Using RAG\n",
    "Generate responses to a query using the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query\n",
    "query = \"Tell me about cats and dogs.\"\n",
    "\n",
    "# Tokenize the query\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate the response\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "# Decode the response\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
